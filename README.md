# Машинное обучение

## Навигация

- [Метрические классификаторы](#Метрические-классификаторы)
  - [Алгоритм ближайшего соседа](#Алгоритм-ближайшего-соседа)
  - [Алгоритм K ближайших соседей](#Алгоритм-K-ближайших-соседей)
  - [Алгоритм k взвешнных ближайших соседей](#Алгоритм-k-взвешнных-ближайших-соседей)
  - [Парзеновское окно (PW)](#Парзеновское-окно-pw)

- [Баейсовские классификаторы](#Баейсовские-классификаторы)
  - [Линии уровня нормального распределения](#Линии-уровня-нормального-распределения)
  - [Наивный нормальный байесовский классификатор](#Наивный-нормальный-байесовский-классификатор)
  - [Подстановочный алгоритм (Plug-in)](#Подстановочный-алгоритм-plug-in)
  - [Линейный дискриминант Фишера (ЛДФ)](#Линейный-дискриминант-Фишера-ЛДФ)
- [Линейные классификаторы](#Линейные-классификаторы)
  - [Алгоритм классификации ADALINE](#Алгоритм-классификации-ADALINE)
  - [Логистическая регрессия](#Логистическая-регрессия)


# Метрические классификаторы

## Алгоритм ближайшего соседа
 **ТЕОРИЯ:**
 
1NN(Алгоритм ближайшего соседа) - относит классифицируемый объект к тому  классу, к которому приналежит его ближайший сосед :
 
 <a href="https://www.codecogs.com/eqnedit.php?latex=w(i,x)&space;=&space;[i\leq&space;1]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w(i,x)&space;=&space;[i\leq&space;1]" title="w(i,x) = [i\leq 1]" /></a>
    
    
  Преимущества метода :
  
   - Простота реализации
   - O(1) - время обучения
     
  Недостатки метода :
  
   - Неустойчивость к погрешностям
   - Отсутсвие параметров, которые можно было бы настраивать по выборке. Алгоритм полностью зависит от того, насколько удачно выбранна метрика p.
   - Низкое качество классификации.
      
 
 **Практическая реализация кода ( decoding ) :**
 
   Вначале задаем метрику. Метрика - это такая функция, которая измеряет меру близости.
    После этого мы задаем функцию nn, которая принимает  объект u, множество объектов xl, метрику и возвращает наиболее близкий объект из множества xl к объекту u. Далее рисуется выборка. После этого надо классифицировать всю плоскость процедурой замощения.
   
   
  **Результат**
  
  ![1NN](https://github.com/peilivanov/peilivanovpavelSMPR/blob/master/%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA2345.PNG)
  
  ## Алгоритм K ближайших соседей
   **ТЕОРИЯ:**
   
   KNN(Алгоритм K ближайших соседей) - относит объект u к тому классу элементов, которого больше среди k ближайших соседедей.
      
      **Attention** (опирается на ранг соседа!)
      
   <a href="https://www.codecogs.com/eqnedit.php?latex=$$w(i,x)&space;=&space;[i&space;\leq&space;k]$$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?$$w(i,x)&space;=&space;[i&space;\leq&space;k]$$" title="$$w(i,x) = [i \leq k]$$" /></a> - метод KNN
      
      
   **Практическая реализация кода ( decoding ) :**
   
   Вначале задаем метрику. 
   Сортируем объекты согласно расстояния до объекта z. Далее создаем матрицу расстояний -> сортируем нашу выборку. 
     
   Применяем метод kNN и сортируем выборку согласно классифицируемого объектаю
   Получаем классы первых k соседей ->  составляем таблицу встречаемости каждого класса -> находим класс, который доминирует среди первых k соседей. Рисуем выборку. Далее идет классификация одного заданного объекта.
   
   
 **Результат**
 
 ![KNN](https://github.com/peilivanov/peilivanovpavelSMPR/blob/master/knnPic.PNG)
 
 ## Выбор параметра k с помощью LOO
 На практике оптимальное k подбирается по критерию скользящего контроля LOO (Leave One Out)
 
 <a href="https://www.codecogs.com/eqnedit.php?latex=LOO(k,&space;X^l&space;)=&space;\sum_{i=1}^{l}&space;\left&space;[&space;a(x_i;&space;X^l\setminus&space;\lbrace&space;x_i&space;\rbrace&space;,&space;k)&space;\neq&space;y_i&space;\right&space;]&space;\rightarrow&space;\min_k&space;." target="_blank"><img src="https://latex.codecogs.com/gif.latex?LOO(k,&space;X^l&space;)=&space;\sum_{i=1}^{l}&space;\left&space;[&space;a(x_i;&space;X^l\setminus&space;\lbrace&space;x_i&space;\rbrace&space;,&space;k)&space;\neq&space;y_i&space;\right&space;]&space;\rightarrow&space;\min_k&space;." title="LOO(k, X^l )= \sum_{i=1}^{l} \left [ a(x_i; X^l\setminus \lbrace x_i \rbrace , k) \neq y_i \right ] \rightarrow \min_k ." /></a>
 
 **Описание**
 
 LOO проверяет алгоритмы на точность. Сначала исключаются по очереди по 1 элементу из выборки -> алгоритм обучается на оставшихся алгоритмах выборки -> извлеченныый элемент классифицируется ->  его ( элемент ) надо вернуть -> далее извлекается следующий и так далее со всеми элементами выборки => после всех проделанных действий можно заметить, где алгоритм ошибается.
 
 
 **Результат**
 
 ![LOO](https://github.com/peilivanov/peilivanovpavelSMPR/blob/master/looPic.PNG)
 
 ## Алгоритм k взвешнных ближайших соседей
 
 **Теория:**
 

 В каждом классе выбирается k ближайших к U объектов, и объект u относится к тому классу, для которого среднее расстояние до k ближайших соседей минимально.
 
 <a href="https://www.codecogs.com/eqnedit.php?latex=$w(i,x)&space;=&space;[i&space;\leq&space;k]w_i$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?$w(i,x)&space;=&space;[i&space;\leq&space;k]w_i$" title="$w(i,x) = [i \leq k]w_i$" /></a> - метод k взвешнных ближайших соседей.
 
 w(i) - строго убывающая последовательность вещественных весов, задающая вклад i-го ссоседа при классификации объекта u.
 
 **Преимущества:**
 
  Главный плюс метода заключается в том, что учитывается степень близости объекта.
  
  **Результат:**
  
  ![kWNN](https://github.com/peilivanov/peilivanovpavelSMPR/blob/master/KWNNpic.png)
  
  ## Парзеновское окно (PW)

Для оценки близости объекта _u_ к классу _y_ алгоритм использует следующую
функцию:

![](http://latex.codecogs.com/svg.latex?%5Clarge%20W%28i%2C%20u%29%20%3D%20K%28%5Cfrac%7B%5Crho%28u%2C%20x%5Ei_u%29%7D%7Bh%7D%29)
, где 
![](http://latex.codecogs.com/svg.latex?%5Clarge%20K%28z%29) — функция ядра.

Чаще всего применяются 5 типов ядер:
- Прямоугольное ![](http://latex.codecogs.com/svg.latex?%5Clarge%20R%28z%29%20%3D%20%5Cfrac%7B1%7D%7B2%7D%20%5B%7Cz%7C%20%5Cleq%201%5D)
- Треугольное ![](http://latex.codecogs.com/svg.latex?%5Clarge%20T%28z%29%20%3D%20%281%20-%20%7Cz%7C%29%20%5Ccdot%20%5B%7Cz%7C%20%5Cleq%201%5D)
- Квартическое ![](http://latex.codecogs.com/svg.latex?%5Clarge%20Q%28z%29%20%3D%20%5Cfrac%7B15%7D%7B16%7D%20%281%20-%20z%5E2%29%5E2%20%5Ccdot%20%5B%7Cz%7C%20%5Cleq%201%5D)
- Епанечниково ![](http://latex.codecogs.com/svg.latex?%5Clarge%20E%28z%29%20%3D%20%5Cfrac%7B3%7D%7B4%7D%20%281%20-%20z%5E2%29%20%5Ccdot%20%5B%7Cz%7C%20%5Cleq%201%5D)
- Гауссовское (нормальное распределение)

Программная реализация ядер:
```
mc.kernel.R = function(r) 0.5 * (abs(r) <= 1) #прямоугольное
mc.kernel.T = function(r)(1 - abs(r)) * (abs(r) <= 1) #треугольное
mc.kernel.Q = function(r)(15 / 16) * (1 - r ^ 2) ^ 2 * (abs(r) <= 1) #квартическое
mc.kernel.E = function(r)(3 / 4) * (1 - r ^ 2) * (abs(r) <= 1) #епанечниково
mc.kernel.G = function(r) dnorm(r) #гауссовское
```
Шаги алгоритма парзеновского окна очень схожи с алгоритмом kwNN, отличаются они различными функциями ядра и наличием такой переменной, как ширина окна.
В программе ядра будут применяться поочередно.
Однако на разницу в качестве классификации они влияют слабо.
Выделяется лишь _гауссовское ядро_.

Карта классификации метода парзеновского окна с использованием квадрического ядра. 

![pw_map](https://user-images.githubusercontent.com/44859059/49109338-7e713080-f29b-11e8-8468-5ab70f77f3ab.png)

Loo для треугольного ядра:


![loopw](https://user-images.githubusercontent.com/44859059/50244640-837c5800-03e1-11e9-91e9-aff25ad3c53b.png)


Loo для Епанечникова ядра:


![loopwep](https://user-images.githubusercontent.com/44859059/50244692-a3138080-03e1-11e9-877f-9660b000c3d1.png)

Плюсы:
- прост в реализации
- хорошее качество классификации при правильно подобраном _h_
- все точки с одинаковым расстоянием будут учитаны

__Минусы:__
- необходимо хранить всю выборку целиком
- бедный набор параметров
- в случае одинаковых весов классов алгоритм выбирает любой
- диапазон параметра h необходимо подбирать самостоятельно, учитывая
плотность расположения точек
  
# Баейсовские классификаторы
  
  **Баейсовский классификатор** — широкий класс алгоритмов классификации, основанный на принципе максимума апостериорной вероятности. Для классифицируемого объекта вычисляются функции правдоподобия каждого из классов, по ним вычисляются апостериорные вероятности классов. Объект относится к тому классу, для которого апостериорная вероятность максимальна. 
  Байесовский подход к классификации основан на теореме, утверждающей, что если плотности распределения каждого из классов известны, то искомый алгоритм можно выписать в явном аналитическом виде. Более того, этот алгоритм оптимален, то есть обладает минимальной вероятностью ошибок.

На практике плотности распределения классов, как правило, не известны. Их приходится оценивать (восстанавливать) по обучающей выборке. В результате байесовский алгоритм перестаёт быть оптимальным, так как восстановить плотность по выборке можно только с некоторой погрешностью. Чем короче выборка, тем выше шансы подогнать распределение под конкретные данные и столкнуться с эффектом переобучения.

Байесовский подход к классификации является одним из старейших, но до сих пор сохраняет прочные позиции в теории распознавания. Он лежит в основе многих достаточно удачных алгоритмов классификации. 

## Линии уровня нормального распределения

 НОРМАЛЬНОЕ РАСПРЕДЕЛЕНИЕ - математическая модель, описывающая распределение случайных (независимых) величин; оно непрерывно, унимодально и симметрично и характеризуется тем, что, по мере удаления от среднего (максимального) значения, частота появления случайной величины падает. При известных параметрах эта модель хорошо апроксимирует статистические данные при достаточно большом объеме (выборке), несмотря на отдельные отклоне.
 
Рассмотрим многомерное нормальное распределение. Пусть <a href="https://www.codecogs.com/eqnedit.php?latex=x\epsilon&space;\mathbb{R}^{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x\epsilon&space;\mathbb{R}^{n}" title="x\epsilon \mathbb{R}^{n}" /></a> вероятностное распределение с плотностью
 
 ![](https://github.com/peilivanov/peilivanovpavelSMPR/blob/master/Bayes/z1.PNG)
 
 называется n-мерным многомерным нормальным (гауссовским) распределением с математическим ожиданием (центром) n и ковариационной матрицей <a href="https://www.codecogs.com/eqnedit.php?latex=\mu&space;\epsilon&space;\mathbb{R}^{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu&space;\epsilon&space;\mathbb{R}^{n}" title="\mu \epsilon \mathbb{R}^{n}" /></a>, которая является симметричной, невырожденной и положительно определенная.
 
 <a href="https://peilivanov.shinyapps.io/LineLevels/">Реализация программы в ShinyAPP</a>
 
 **Пример:**
 ![](https://github.com/peilivanov/peilivanovpavelSMPR/blob/master/Bayes/linelev.PNG)
 
 
 ## Наивный нормальный байесовский классификатор
 
 Наивный байесовский алгоритм – это алгоритм классификации, основанный на теореме Байеса с допущением о независимости признаков. Другими словами, НБА предполагает, что наличие какого-либо признака в классе не связано с наличием какого-либо другого признака. Например, фрукт может считаться яблоком, если он красный, круглый и его диаметр составляет порядка 8 сантиметров. Даже если эти признаки зависят друг от друга или от других признаков, в любом случае они вносят независимый вклад в вероятность того, что этот фрукт является яблоком. В связи с таким допущением алгоритм называется «наивным».

Модели на основе НБА достаточно просты и крайне полезны при работе с очень большими наборами данных. При своей простоте НБА способен превзойти даже некоторые сложные алгоритмы классификации.

Теорема Байеса позволяет рассчитать апостериорную вероятность P(c|x) на основе P(c), P(x) и P(x|c).


<a href="https://www.codecogs.com/eqnedit.php?latex=P(c|x)=\frac{P(x|c)P(c))}{P(x)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(c|x)=\frac{P(x|c)P(c))}{P(x)}" title="P(c|x)=\frac{P(x|c)P(c))}{P(x)}" /></a>
 
 
 На рисунке выше:

P(c|x) – апостериорная вероятность данного класса c (т.е. данного значения целевой переменной) при данном значении признака x.
P(c) – априорная вероятность данного класса.
P(x|c) – правдоподобие, т.е. вероятность данного значения признака при данном классе.
P(x) – априорная вероятность данного значения признака.

**Как работает наивный байесовский алгоритм?**

Ниже представлен обучающий набор данных, содержащий один признак «Погодные условия» (weather) и целевую переменную «Игра» (play), которая обозначает возможность проведения матча. На основе погодных условий мы должны определить, состоится ли матч. Чтобы сделать это, необходимо выполнить следующие шаги.


Шаг 1. Преобразуем набор данных в частотную таблицу (frequency table).


Шаг 2. Создадим таблицу правдоподобия (likelihood table), рассчитав соответствующие вероятности. Например, вероятность облачной погоды (overcast) составляет 0,29, а вероятность того, что матч состоится (yes) – 0,64.


Шаг 3. С помощью теоремы Байеса рассчитаем апостериорную вероятность для каждого класса при данных погодных условиях. Класс с наибольшей апостериорной вероятностью будет результатом прогноза.


![](https://github.com/peilivanov/peilivanovpavelSMPR/blob/master/Bayes/Bayes_41.png)

Sunny – Солнечная погода
Rainy – Дождливая погода
Overcast – Облачная погода

**Пример вывода в програмной реализации:**

 <a href="https://peilivanov.shinyapps.io/Naive_Normal_Bayes/">Реализация программы в ShinyAPP</a>

![](https://github.com/peilivanov/peilivanovpavelSMPR/blob/master/Bayes/nai2.PNG)

   .
## Подстановочный алгоритм (Plug-in)

__Подстановочный алгоритм__ относится к нормальному дискриминантному анализу.

Чтобы узнать _плотности распределения классов_, алогритм восстанавливает
неизвестные параметры
![](http://latex.codecogs.com/svg.latex?%5Cmu%2C%20%5CSigma)
по следующим формулам для каждого класса _y_ :

![](http://latex.codecogs.com/svg.latex?%5Chat%7B%5Cmu%7D%20%3D%20%5Cfrac%7B1%7D%7Bl_y%7D%20%24%24%5Csum_%7Bi%20%3D%201%7D%5E%7Bl_y%7D%20x_i%24%24)


![](http://latex.codecogs.com/svg.latex?%5Chat%7B%5CSigma%7D%20%3D%20%5Cfrac%7B1%7D%7Bl_y%20-%201%7D%20%24%24%5Csum_%7Bi%20%3D%201%7D%5E%7Bl_y%7D%20%28x_i%20-%20%5Chat%7B%5Cmu%7D%29%28x_i%20-%20%5Chat%7B%5Cmu%7D%29%5ET).

Недостатки подстановочного алгоритма.

• Если длина выборки меньше размерности пространства, ℓy < n, то матрица Σy становится вырожденной, поскольку её ранг не может превышать ℓy . В этом случае обратная матрица не существует и метод вообще не применим.

• Даже если длина выборки больше размерности пространства, ℓy > n, матрица Σy всё равно может оказаться вырожденной. Это происходит, когда признаки оказываются линейно зависимыми.

•Если функции правдоподобия классов существенно отличаются от гауссовских, то методы нормального дискриминантного анализа могут приводить к алгоритмам низкого качества. В частности, когда имеются номинальные признаки, принимающие дискретные значения, или когда классы распадаются на изолированные сгустки.

## Линейный дискриминант Фишера (ЛДФ)

__ЛДФ__ основан на __подстановочном алгоритме__ с предположением,
что ковариационные матрицы классов равны. Отсюда следует, что
разделяющая поверхность вырождается в прямую. Это условие в
__plug-in__ не выполнялось, так как разделяющая поверхность все равно
была квадратичной (хоть и приближенной к прямой). Отсюда следует,
что __ЛДФ__ должен иметь более высокое качество классификации при
одинаковых ковариационных матрицах.

Программно алгоритм отличается от __подстановочного__ пересчетом
_ковариационной матрицы_ и поиском _разделяющей поверхности_.

# Линейные классификаторы

Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на два полупространства, в каждом из которых прогнозируется одно из двух значений целевого класса.
Если это можно сделать без ошибок, то обучающая выборка называется линейно разделимой.

![](https://hsto.org/files/77b/af8/55c/77baf855c84a4ba9a851060ec7ae6ec8.png)

## Алгоритм классификации ADALINE

даптивный линейный элемент(Адаптивный линейный нейрон или ADALINE) – частный случай линейного классификатора или искусственной нейронной сети с одним слоем. Был предложен Видроу и Хоффом в 1960 году, развивая математическую модель нейрона МакКаллока–Питтса.

Общая схема работы ADALINE

Схема работы ADALINE несколько напоминает работу биологического нейрона:

![](https://i1.wp.com/79811.selcdn.ru/ai_science/2016/08/300px-Mat_model_neirona.png?zoom=1.25&w=618&ssl=1)
модель работы нейрона

На вход подаётся вектор импульсов xn ,состоящий из n числовых признаков. Внутри нейрона импульсы складываются с некоторыми весами wj, j = 1..n и, если суммарный импульс S(x) =textstylesum_{j=1}^n w_jx_j превысит порог активации w0, то нейрон возбуждается и выдаёт некоторое значение a(x) = S(x) – w0.

Если добавить фиктивный импульс-признак x0 ≡ -1 и ему сопоставить вес w0 – порог активации, то формулу выхода Y(x) можно выписать более компактно:
a(x) =(w,x) , где (w,x) – скалярное произведение, w и x – векторы весов и импульсов-признаков соответственно.

Обучение ADALINE
Пусть дана обучающая выборка: множество входных значений X и множество выходящих значений Y, такие что каждому входу xj соответствует yj – выход, j = 1..m. Необходимо по этим данным построить ADALINE, которая допускает наименьшее количество ошибок на этой обучающей выборке. Обучение ADALINE заключается в подборе “наилучших” значений вектора весов w. Какие значение весов лучше определяет функционал потерь.В ADALINE используется функционал, предложенный Видроу и Хоффом, L(a,x) = (a – y)2. Таким образом необходимо минимизировать функционал Q(w):
Q(w) , = , sum_{i=1}^m (a(x_i, w) - y_i)^2 to min_w
Применим метод градиентного спуска, тогда следующее значение будет иметь вид: w , {:=} , w , - , eta nabla Q(w) = w - eta ( (w,x) - y)*x  ,где eta – темп обучения.

 

**Схема обучение ADALINE**

Вход:

X^m – обучающая выборка из m элементов
eta – темп обучения
lambda – параметр сглаживания функционала Q
Выход:

Вектор весов w
Тело:

1. Инициализировать веса w_j ; j = 0, dots, n;
2. Инициализировать начальную оценку функционала:Q , {:=} , sum_{i=1}^m (a(x_i, w) - y_i)^2;
3. Повторять:
 1) Выбрать объект x_i из X^l (например, случайным образом);
 2) Вычислить ошибку:varepsilon_i , {:=} , ((x_i, w)- y_i)^2;
 3) Сделать шаг градиентного спуска:w , {:=} w - eta ( (x_i, w) - y_i)*x_i;
 4) Оценить значение функционала:Q , {:=} , (1 , - , lambda)Q , + , lambdavarepsilon_i;
4. Пока значение Q не стабилизируется и/или веса w не перестанут изменяться.

Пример работы алгоритма:

![](https://github.com/peilivanov/peilivanovpavelSMPR/blob/master/Adaliine.png)

## Логистическая регрессия


Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим "умением" – прогнозировать вероятность p+  отнесения примера Xi  к классу "+":

![](https://github.com/peilivanov/peilivanovpavelSMPR/blob/master/formus.PNG)

является __оптимальный байесовским классификатором__ из-за своих довольно
сильных вероятностных предположений.

Имеет _логистическую функцию потерь_
![](http://latex.codecogs.com/svg.latex?%5Cmathcal%7BL%7D%28M%29%20%3D%20%5Clog_2%281%20&plus;%20e%5E%7B-M%7D%29)
и _логистическое_ правило обновления весов
![](http://latex.codecogs.com/svg.latex?w%20%3A%3D%20w&plus;%5Ceta%20y_ix_i%5Csigma%28-%5Clangle%20w%2Cx_i%20%5Crangle%20y_i%29)
, где
![](http://latex.codecogs.com/svg.latex?%5Csigma%28z%29%3D%5Cfrac%7B1%7D%7B1&plus;e%5E%7B-z%7D%7D)
– _сигмоидная функция_.

Пример работы:

![](https://user-images.githubusercontent.com/44859059/51426440-fb65ba00-1bfb-11e9-9422-b915b860c68c.png)
